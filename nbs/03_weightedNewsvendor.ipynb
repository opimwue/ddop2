{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccc8a9-8a7b-483c-9379-14db1571b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0c691-8934-45ac-aa8e-458b1ccdadd7",
   "metadata": {},
   "source": [
    "# Weighted Newsvendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048b4c8-599d-47e6-b7bc-991945e480ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp newsvendor._WeightedNewsvendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b69cbd-4de4-4b1a-bb5b-4695052a5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043dda5-8a7b-4530-9448-0a00e38f4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from ddop2.newsvendor._base import BaseNewsvendor, DataDrivenMixin\n",
    "from ddop2.utils.validation import check_cu_co\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils.validation import check_is_fitted, check_array\n",
    "from scipy.spatial import distance_matrix\n",
    "import mpmath as mp\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6eb384-2278-4baa-ab49-84a88876d722",
   "metadata": {},
   "source": [
    "## Base Weighted Newsvendor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cd68b-f8e8-40a9-87d2-a158b5dbfb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class BaseWeightedNewsvendor(BaseNewsvendor, DataDrivenMixin, ABC):\n",
    "    \n",
    "    \"\"\"Base class for weighted newsvendor.\n",
    "    Warning: This class should not be used directly.\n",
    "    Use derived classes instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 cu=None,\n",
    "                 co=None\n",
    "                 ):\n",
    "        self.cu = cu\n",
    "        self.co = co\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the estimator to the training set (X,y)\"\"\"\n",
    "\n",
    "        X, y = self._validate_data(X, y, multi_output=True)\n",
    "\n",
    "        self._get_fitted_model(X, y)\n",
    "\n",
    "        if y.ndim == 1:\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "\n",
    "        # Training data\n",
    "        self.y_ = y\n",
    "        self.X_ = X\n",
    "        self.n_samples_ = y.shape[0]\n",
    "\n",
    "        # Determine output settings\n",
    "        self.n_outputs_ = y.shape[1]\n",
    "        self.n_features_ = X.shape[1]\n",
    "\n",
    "        # Check and format under- and overage costs\n",
    "        self.cu_, self.co_ = check_cu_co(self.cu, self.co, self.n_outputs_)\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_fitted_model(self, X, y):\n",
    "        \"\"\"Initialise the underlying model\"\"\"\n",
    "        \n",
    "\n",
    "    @abstractmethod\n",
    "    def _calc_weights(self, sample):\n",
    "        \"\"\"Calculate the sample weights\"\"\"\n",
    "        \n",
    "\n",
    "    def _validate_X_predict(self, X):\n",
    "        \"\"\"Validate X whenever one tries to predict\"\"\"\n",
    "\n",
    "        X = check_array(X)\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        if self.n_features_ != n_features:\n",
    "            raise ValueError(\"Number of features of the model must match the input. \"\n",
    "                             \"Model n_features is %s and input n_features is %s \"\n",
    "                             % (self.n_features_, n_features))\n",
    "        return X\n",
    "\n",
    "    \n",
    "    def _findQ(self, weights, weightPosIndices):\n",
    "        \"\"\"Calculate the optimal order quantity q\"\"\"\n",
    "        \n",
    "        y = self.y_\n",
    "        yWeightPos = y[weightPosIndices]\n",
    "        \n",
    "        q = []\n",
    "        \n",
    "        for i in range(self.n_outputs_):\n",
    "            serviceLevel = self.cu_[i] / (self.cu_[i] + self.co_[i])\n",
    "            \n",
    "            indicesYSort = np.argsort(yWeightPos[:, i])\n",
    "            ySorted = yWeightPos[indicesYSort, i]\n",
    "            \n",
    "            distributionFunction = np.cumsum(weights[indicesYSort])\n",
    "            decisionIndex = np.where(distributionFunction >= serviceLevel)[0][0]\n",
    "            \n",
    "            q.append(ySorted[decisionIndex])\n",
    "        \n",
    "        return q\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict value for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples to predict.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        y : array-like of shape (n_samples, n_outputs)\n",
    "            The predicted values\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._validate_X_predict(X)\n",
    "        check_is_fitted(self)        \n",
    "       \n",
    "        weightsDataList = [self._calc_weights(row) for row in X]\n",
    "        pred = [self._findQ(weights, weightPosIndices) \n",
    "                for weights, weightPosIndices in weightsDataList]\n",
    "        pred = np.array(pred)        \n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e67e1-c06b-473b-bbb9-151906c538fc",
   "metadata": {},
   "source": [
    "## Decision Tree Weighted Newsvendor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6e7c7-0dcd-4b98-b7fc-88ed9994d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DecisionTreeWeightedNewsvendor(BaseWeightedNewsvendor):\n",
    "    \n",
    "    \"\"\"A decision tree weighted SAA model to solve the newsvendor problem.\n",
    "\n",
    "    This class implements the approach described in [5] with a weight function\n",
    "    based on decision tree regression. To build the tree the\n",
    "    DecisionTreeRegressor from scikit-learn is used [6].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cu : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The underage costs per unit. If None, then underage costs are one\n",
    "        for each target variable\n",
    "    co : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The overage costs per unit. If None, then overage costs are one\n",
    "        for each target variable\n",
    "    criterion : {\"mse\", \"friedman_mse\", \"mae\"}, default=\"mse\"\n",
    "        The function to measure the quality of a split. Supported criteria\n",
    "        are \"mse\" for the mean squared error, which is equal to variance\n",
    "        reduction as feature selection criterion and minimizes the L2 loss\n",
    "        using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
    "        squared error with Friedman's improvement score for potential splits,\n",
    "        and \"mae\" for the mean absolute error, which minimizes the L1 loss\n",
    "        using the median of each terminal node.\n",
    "    splitter : {\"best\", \"random\"}, default=\"best\"\n",
    "        The strategy used to choose the split at each node. Supported\n",
    "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
    "        the best random split.\n",
    "    max_depth : int, default=None\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "    min_samples_split : int or float, default=2\n",
    "        The minimum number of samples required to split an internal node:\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "    min_samples_leaf : int or float, default=1\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "    min_weight_fraction_leaf : float, default=0.0\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "    max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
    "        The number of features to consider when looking for the best split:\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=n_features`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Controls the randomness of the estimator. The features are always\n",
    "        randomly permuted at each split, even if ``splitter`` is set to\n",
    "        ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
    "        select ``max_features`` at random at each split before finding the best\n",
    "        split among them. But the best found split may vary across different\n",
    "        runs, even if ``max_features=n_features``. That is the case, if the\n",
    "        improvement of the criterion is identical for several splits and one\n",
    "        split has to be selected at random. To obtain a deterministic behaviour\n",
    "        during fitting, ``random_state`` has to be fixed to an integer.\n",
    "        See :term:`Glossary <random_state>` for details.\n",
    "    max_leaf_nodes : int, default=None\n",
    "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "    min_impurity_decrease : float, default=0.0\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "        The weighted impurity decrease equation is the following::\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "    ccp_alpha : non-negative float, default=0.0\n",
    "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "        subtree with the largest cost complexity that is smaller than\n",
    "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
    "        :ref:`minimal_cost_complexity_pruning` for details.\n",
    "\n",
    "    Attributes\n",
    "    ---------\n",
    "    train_leaf_indices_ : array of shape (n_samples,)\n",
    "        The leaf indices of the training samples\n",
    "    y_ : array of shape (n_samples, n_outputs)\n",
    "        The y training data\n",
    "    cu_ : ndarray, shape (n_outputs,)\n",
    "        Validated underage costs.\n",
    "    co_ : ndarray, shape (n_outputs,)\n",
    "        Validated overage costs.\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "    n_samples_ : int\n",
    "        The number of samples when ``fit`` is performed.\n",
    "    model_ : DecisionTreeRegressor\n",
    "        The DecisionTreeRegressor used to calculate the sample weights\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for the parameters controlling the size of the trees\n",
    "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "    unpruned trees which can potentially be very large on some data sets. To\n",
    "    reduce memory consumption, the complexity and size of the trees should be\n",
    "    controlled by setting those parameter values.\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
    "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
    "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
    "           Learning\", Springer, 2009.\n",
    "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
    "           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
    "    .. [5] Bertsimas, Dimitris, and Nathan Kallus, \"From predictive to prescriptive analytics.\"\n",
    "           arXiv preprint arXiv:1402.5481 (2014).\n",
    "    .. [6] scikit-learn, RandomForestRegressor,\n",
    "           <https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_classes.py>\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from ddop.datasets import load_yaz\n",
    "    >>> from ddop.newsvendor import DecisionTreeWeightedNewsvendor\n",
    "    >>> from sklearn.model_selection import train_test_split\n",
    "    >>> X, Y = load_yaz(include_prod=['STEAK'],return_X_y=True)\n",
    "    >>> cu,co = 15,10\n",
    "    >>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=False, random_state=0)\n",
    "    >>> mdl = DecisionTreeWeightedNewsvendor(cu, co, random_state=0)\n",
    "    >>> mdl.fit(X_train, Y_train)\n",
    "    >>> score(X_test, Y_test)\n",
    "    TODO: Add output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cu=None,\n",
    "                 co=None,\n",
    "                 criterion=\"mse\",\n",
    "                 splitter=\"best\",\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.,\n",
    "                 max_features=None,\n",
    "                 random_state=None,\n",
    "                 max_leaf_nodes=None,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 ccp_alpha=0.0):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.random_state = random_state\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        super().__init__(\n",
    "            cu=cu,\n",
    "            co=co\n",
    "        )\n",
    "        \n",
    "\n",
    "    def _get_fitted_model(self, X, y):\n",
    "        model = DecisionTreeRegressor(\n",
    "            criterion=self.criterion,\n",
    "            splitter=self.splitter,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "            max_features=self.max_features,\n",
    "            max_leaf_nodes=self.max_leaf_nodes,\n",
    "            min_impurity_decrease=self.min_impurity_decrease,\n",
    "            random_state=self.random_state,\n",
    "            ccp_alpha=self.ccp_alpha,\n",
    "        )\n",
    "\n",
    "        self.model_ = model.fit(X, y)\n",
    "        self.train_leaf_indices_ = model.apply(X)\n",
    "\n",
    "    \n",
    "    def _calc_weights(self, sample):\n",
    "        sample_leaf_indices = self.model_.apply([sample])\n",
    "        n = np.sum(sample_leaf_indices == self.train_leaf_indices_, axis=0)\n",
    "        weights = (sample_leaf_indices == self.train_leaf_indices_) / n\n",
    "        \n",
    "        weightPosIndex = np.where(weights > 0)[0]\n",
    "        weightsPos = weights[weightPosIndex]\n",
    "\n",
    "        return (weightsPos, weightPosIndex)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the estimator from the training set (X,y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples, n_features)\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        self : DecisionTreeWeightedNewsvendor\n",
    "            Fitted estimator\n",
    "        \"\"\"\n",
    "\n",
    "        super().fit(X, y)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25128f0c-84f8-45c7-a02e-72991c5265d4",
   "metadata": {},
   "source": [
    "## Random Forest Weighted Newsvendor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557286c6-b017-4ad2-9186-236cef4cce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class RandomForestWeightedNewsvendor(BaseWeightedNewsvendor):\n",
    "    \n",
    "    \"\"\"A random forest weighted SAA model to solve the newsvendor problem.\n",
    "\n",
    "    This class implements the approach described in [3] with a weight function\n",
    "    based on random forest regression. To build the random forest the\n",
    "    RandomForestRegressor from scikit-learn is used [4].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cu : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The underage costs per unit. If None, then underage costs are one\n",
    "        for each target variable\n",
    "    co : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The overage costs per unit. If None, then overage costs are one\n",
    "        for each target variable\n",
    "    criterion: {\"squared_error\", \"friedman_mse, \"mae\"}, default=\"squared_error\"\n",
    "        The function to measure the quality of a split. Supported criteria\n",
    "        are \"mse\" for the mean squared error, which is equal to variance\n",
    "        reduction as feature selection criterion and minimizes the L2 loss\n",
    "        using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
    "        squared error with Friedman's improvement score for potential splits,\n",
    "        and \"mae\" for the mean absolute error, which minimizes the L1 loss\n",
    "        using the median of each terminal node.\n",
    "    n_estimators : int, default=100\n",
    "        The number of trees in the forest.\n",
    "    max_depth : int, default=None\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "    weight_function : {\"w1\", \"w2\"}, default=\"w1\"\n",
    "        Indicates how to determine the sample weights. If set to \"w1\" the weight\n",
    "        function corresponds to the one described in [3]. If set to \"w2\" the\n",
    "        weight function described in [5] will be used.\n",
    "    min_samples_split : int or float, default=2\n",
    "        The minimum number of samples required to split an internal node:\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "    min_samples_leaf : int or float, default=1\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "    min_weight_fraction_leaf : float, default=0.0\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "    max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
    "        The number of features to consider when looking for the best split:\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=n_features`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "    max_leaf_nodes : int, default=None\n",
    "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "    min_impurity_decrease : float, default=0.0\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "        The weighted impurity decrease equation is the following::\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "    bootstrap : bool, default=True\n",
    "        Whether bootstrap samples are used when building trees. If False, the\n",
    "        whole dataset is used to build each tree.\n",
    "    oob_score : bool, default=False\n",
    "        whether to use out-of-bag samples to estimate\n",
    "        the R^2 on unseen data.\n",
    "    n_jobs : int, default=None\n",
    "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
    "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
    "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "        context. ``-1`` means using all processors.\n",
    "    random_state : int or RandomState, default=None\n",
    "        Controls both the randomness of the bootstrapping of the samples used\n",
    "        when building trees (if ``bootstrap=True``) and the sampling of the\n",
    "        features to consider when looking for the best split at each node\n",
    "        (if ``max_features < n_features``).\n",
    "    verbose : int, default=0\n",
    "        Controls the verbosity when fitting and predicting.\n",
    "    warm_start : bool, default=False\n",
    "        When set to ``True``, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "        new forest.\n",
    "    ccp_alpha : non-negative float, default=0.0\n",
    "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "        subtree with the largest cost complexity that is smaller than\n",
    "        ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
    "    max_samples : int or float, default=None\n",
    "        If bootstrap is True, the number of samples to draw from X\n",
    "        to train each base estimator.\n",
    "        - If None (default), then draw `X.shape[0]` samples.\n",
    "        - If int, then draw `max_samples` samples.\n",
    "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "          `max_samples` should be in the interval `(0, 1)`.\n",
    "\n",
    "    Attributes\n",
    "    ---------\n",
    "    train_leaf_indices_ : array of shape (n_samples,)\n",
    "        The leaf indices of the training samples\n",
    "    y_ : array of shape (n_samples, n_outputs)\n",
    "        The y training data\n",
    "    cu_ : ndarray, shape (n_outputs,)\n",
    "        Validated underage costs.\n",
    "    co_ : ndarray, shape (n_outputs,)\n",
    "        Validated overage costs.\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "    n_samples_ : int\n",
    "        The number of samples when ``fit`` is performed.\n",
    "    model_ : RandomForestRegressor\n",
    "        The RandomForestRegressor used to calculate the sample weights\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for the parameters controlling the size of the trees\n",
    "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "    unpruned trees which can potentially be very large on some data sets. To\n",
    "    reduce memory consumption, the complexity and size of the trees should be\n",
    "    controlled by setting those parameter values.\n",
    "    The features are always randomly permuted at each split. Therefore,\n",
    "    the best found split may vary, even with the same training data,\n",
    "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
    "    of the criterion is identical for several splits enumerated during the\n",
    "    search of the best split. To obtain a deterministic behaviour during\n",
    "    fitting, ``random_state`` has to be fixed.\n",
    "\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
    "    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
    "           trees\", Machine Learning, 63(1), 3-42, 2006.\n",
    "    .. [3] Bertsimas, Dimitris, and Nathan Kallus, \"From predictive to prescriptive analytics.\"\n",
    "           arXiv preprint arXiv:1402.5481 (2014).\n",
    "    .. [4] scikit-learn, RandomForestRegressor,\n",
    "           <https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/ensemble/_forest.py>\n",
    "    .. [5] Scornet, Erwan. \"Random forests and kernel methods.\"\n",
    "           IEEE Transactions on Information Theory 62.3 (2016): 1485-1500.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from ddop.datasets import load_yaz\n",
    "    >>> from ddop.newsvendor import RandomForestWeightedNewsvendor\n",
    "    >>> from sklearn.model_selection import train_test_split\n",
    "    >>> X, Y = load_yaz(include_prod=['STEAK'],return_X_y=True)\n",
    "    >>> cu,co = 15,10\n",
    "    >>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=False, random_state=0)\n",
    "    >>> mdl = RandomForestWeightedNewsvendor(cu, co, random_state=0)\n",
    "    >>> mdl.fit(X_train, Y_train)\n",
    "    >>> score(X_test, Y_test)\n",
    "    TODO: Add output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cu=None,\n",
    "                 co=None,\n",
    "                 criterion=\"squared_error\",\n",
    "                 n_estimators=100,\n",
    "                 max_depth=None,\n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf=1,\n",
    "                 min_weight_fraction_leaf=0.,\n",
    "                 max_features=1,\n",
    "                 max_leaf_nodes=None,\n",
    "                 min_impurity_decrease=0.,\n",
    "                 bootstrap=True,\n",
    "                 oob_score=False,\n",
    "                 n_jobs=None,\n",
    "                 random_state=None,\n",
    "                 verbose=0,\n",
    "                 warm_start=False,\n",
    "                 ccp_alpha=0.0,\n",
    "                 max_samples=None,\n",
    "                 weight_function=\"w1\"\n",
    "                 ):\n",
    "        self.criterion = criterion\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.bootstrap = bootstrap\n",
    "        self.oob_score = oob_score\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        self.max_samples = max_samples\n",
    "        self.weight_function = weight_function\n",
    "        super().__init__(\n",
    "            cu=cu,\n",
    "            co=co\n",
    "        )\n",
    "        \n",
    "\n",
    "    def _get_fitted_model(self, X, y):\n",
    "        model = RandomForestRegressor(\n",
    "            criterion=self.criterion,\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "            max_features=self.max_features,\n",
    "            max_leaf_nodes=self.max_leaf_nodes,\n",
    "            min_impurity_decrease=self.min_impurity_decrease,\n",
    "            bootstrap=self.bootstrap,\n",
    "            oob_score=self.oob_score,\n",
    "            n_jobs=self.n_jobs,\n",
    "            random_state=self.random_state,\n",
    "            verbose=self.verbose,\n",
    "            warm_start=self.warm_start,\n",
    "            ccp_alpha=self.ccp_alpha,\n",
    "            max_samples=self.max_samples\n",
    "        )\n",
    "\n",
    "        self.model_ = model.fit(X, y)\n",
    "        self.train_leaf_indices_ = model.apply(X)\n",
    "        \n",
    "\n",
    "    def _calc_weights(self, sample):\n",
    "        sample_leaf_indices = self.model_.apply([sample])\n",
    "        if self.weight_function == \"w1\":\n",
    "            n = np.sum(sample_leaf_indices == self.train_leaf_indices_, axis=0)\n",
    "            treeWeights = (sample_leaf_indices == self.train_leaf_indices_) / n\n",
    "            weights = np.sum(treeWeights, axis=1) / self.n_estimators\n",
    "        else:\n",
    "            n = np.sum(sample_leaf_indices == self.train_leaf_indices_)\n",
    "            treeWeights = (sample_leaf_indices == self.train_leaf_indices_) / n\n",
    "            weights = np.sum(treeWeights, axis=1)\n",
    "        \n",
    "        weightPosIndex = np.where(weights > 0)[0]\n",
    "        weightsPos = weights[weightPosIndex]\n",
    "\n",
    "        return (weightsPos, weightPosIndex)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the estimator from the training set (X,y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples, n_features)\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        self : RandomForestWeightedNewsvendor\n",
    "            Fitted estimator\n",
    "        \"\"\"\n",
    "\n",
    "        super().fit(X, y)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dfb9ce-0ebc-4659-b8d7-137c860c00e8",
   "metadata": {},
   "source": [
    "## K-Nearest-Neighbors Weighted Newsvendor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe589e86-dd5e-472a-a1e5-c4f7ae1fc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class KNeighborsWeightedNewsvendor(BaseWeightedNewsvendor):\n",
    "    \n",
    "    \"\"\"A k-nearest-neighbor weighted SAA model to solve the newsvendor problem\n",
    "\n",
    "    This class implements the approach described in [3] with a weight function\n",
    "    based k-nearest-neighbor regression. To determine the k-nearest-neighbors\n",
    "    NearestNeighbors from scikit-learn is used [4].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cu : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The underage costs per unit. If None, then underage costs are one\n",
    "        for each target variable\n",
    "    co : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The overage costs per unit. If None, then overage costs are one\n",
    "        for each target variable\n",
    "    n_neighbors : int, default=5\n",
    "        Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "    radius : float, default=1.0\n",
    "        Range of parameter space to use by default for :meth:`radius_neighbors`\n",
    "        queries.\n",
    "    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
    "        Algorithm used to compute the nearest neighbors:\n",
    "        - 'ball_tree' will use :class:`BallTree`\n",
    "        - 'kd_tree' will use :class:`KDTree`\n",
    "        - 'brute' will use a brute-force search.\n",
    "        - 'auto' will attempt to decide the most appropriate algorithm\n",
    "          based on the values passed to :meth:`fit` method.\n",
    "        Note: fitting on sparse input will override the setting of\n",
    "        this parameter, using brute force.\n",
    "    leaf_size : int, default=30\n",
    "        Leaf size passed to BallTree or KDTree.  This can affect the\n",
    "        speed of the construction and query, as well as the memory\n",
    "        required to store the tree.  The optimal value depends on the\n",
    "        nature of the problem.\n",
    "    metric : str or callable, default='minkowski'\n",
    "        the distance metric to use for the tree.  The default metric is\n",
    "        minkowski, and with p=2 is equivalent to the standard Euclidean\n",
    "        metric. See the documentation of :class:`DistanceMetric` for a\n",
    "        list of available metrics.\n",
    "        If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
    "        must be square during fit. X may be a :term:`sparse graph`,\n",
    "        in which case only \"nonzero\" elements may be considered neighbors.\n",
    "    p : int, default=2\n",
    "        Parameter for the Minkowski metric from\n",
    "        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is\n",
    "        equivalent to using manhattan_distance (l1), and euclidean_distance\n",
    "        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "    metric_params : dict, default=None\n",
    "        Additional keyword arguments for the metric function.\n",
    "    n_jobs : int, default=None\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors.\n",
    "\n",
    "    Attributes\n",
    "    ---------\n",
    "    y_ : array of shape (n_samples, n_outputs)\n",
    "        The y training data\n",
    "    cu_ : ndarray, shape (n_outputs,)\n",
    "        Validated underage costs.\n",
    "    co_ : ndarray, shape (n_outputs,)\n",
    "        Validated overage costs.\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "    n_samples_ : int\n",
    "        The number of samples when ``fit`` is performed.\n",
    "    model_ : NearestNeighbors\n",
    "        The underlying model used to calculate the sample weights\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Bertsimas, Dimitris, and Nathan Kallus, \"From predictive to prescriptive analytics.\"\n",
    "           arXiv preprint arXiv:1402.5481 (2014).\n",
    "    .. [2] scikit-learn, NearestNeighbors,\n",
    "           <https://github.com/scikit-learn/scikit-learn/blob/fd237278e/sklearn/neighbors/_unsupervised.py>\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from ddop.datasets import load_yaz\n",
    "    >>> from ddop.newsvendor import KNeighborsWeightedNewsvendor\n",
    "    >>> from sklearn.model_selection import train_test_split\n",
    "    >>> X, Y = load_yaz(include_prod=['STEAK'],return_X_y=True)\n",
    "    >>> cu,co = 15,10\n",
    "    >>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=False, random_state=0)\n",
    "    >>> mdl = KNeighborsWeightedNewsvendor(cu, co, random_state=0)\n",
    "    >>> mdl.fit(X_train, Y_train)\n",
    "    >>> score(X_test, Y_test)\n",
    "    TODO: Add output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cu=None,\n",
    "                 co=None,\n",
    "                 n_neighbors=5,\n",
    "                 radius=1.0,\n",
    "                 algorithm='auto',\n",
    "                 leaf_size=30,\n",
    "                 metric='minkowski',\n",
    "                 p=2,\n",
    "                 metric_params=None,\n",
    "                 n_jobs=None\n",
    "                 ):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.radius = radius\n",
    "        self.algorithm = algorithm\n",
    "        self.leaf_size = leaf_size\n",
    "        self.metric = metric\n",
    "        self.p = p\n",
    "        self.metric_params = metric_params\n",
    "        self.n_jobs = n_jobs\n",
    "        super().__init__(\n",
    "            cu=cu,\n",
    "            co=co\n",
    "        )\n",
    "        \n",
    "\n",
    "    def _get_fitted_model(self, X, y=None):\n",
    "        model = NearestNeighbors(\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            radius=self.radius,\n",
    "            algorithm=self.algorithm,\n",
    "            leaf_size=self.leaf_size,\n",
    "            metric=self.metric,\n",
    "            p=self.p,\n",
    "            metric_params=self.metric_params,\n",
    "            n_jobs=self.n_jobs\n",
    "        )\n",
    "\n",
    "        self.model_ = model.fit(X)\n",
    "        \n",
    "\n",
    "    def _calc_weights(self, sample):\n",
    "        neighbors = self.model_.kneighbors([sample], return_distance=False)[0]\n",
    "        weightsPos = np.array([1 / self.n_neighbors for i in range(len(neighbors))])\n",
    "        \n",
    "        return (weightsPos, neighbors)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the estimator from the training set (X,y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples, n_features)\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        self : KNeighborsWeightedNewsvendor\n",
    "            Fitted estimator\n",
    "        \"\"\"\n",
    "\n",
    "        super().fit(X, y)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b49695-b30b-40ba-a210-912e9832b06f",
   "metadata": {},
   "source": [
    "## Gaussian Weighted Newsvendor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e0997-6a94-4b05-bece-a185f7e6e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class GaussianWeightedNewsvendor(BaseWeightedNewsvendor):\n",
    "    \n",
    "    \"\"\"A gaussian kernel weighted SAA model to solve the newsvendor problem\n",
    "\n",
    "    This class implements the approach described in [1] with a gaussian kernel weight function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cu : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The underage costs per unit. If None, then underage costs are one\n",
    "        for each target variable\n",
    "    co : {array-like of shape (n_outputs,), Number or None}, default=None\n",
    "        The overage costs per unit. If None, then overage costs are one\n",
    "        for each target variable\n",
    "    kernel_bandwidth: float or int, default=1\n",
    "        The bandwidth of the kernel function\n",
    "\n",
    "    Attributes\n",
    "    ---------\n",
    "    X_ : array of shape (n_samples, n_features)\n",
    "        The X training data\n",
    "    y_ : array of shape (n_samples, n_outputs)\n",
    "        The y training data\n",
    "    cu_ : ndarray, shape (n_outputs,)\n",
    "        Validated underage costs.\n",
    "    co_ : ndarray, shape (n_outputs,)\n",
    "        Validated overage costs.\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "    n_samples_ : int\n",
    "        The number of samples when ``fit`` is performed.\n",
    "    kernel_ :\n",
    "        The kernel object\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Gah-Yi Ban, Cynthia Rudin, \"The Big Data Newsvendor: Practical Insights from\n",
    "    Machine Learning\", 2018.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from ddop.datasets import load_yaz\n",
    "    >>> from ddop.newsvendor import GaussianWeightedNewsvendor\n",
    "    >>> from sklearn.model_selection import train_test_split\n",
    "    >>> X, Y = load_yaz(include_prod=['STEAK'],return_X_y=True)\n",
    "    >>> cu,co = 15,10\n",
    "    >>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, shuffle=False, random_state=0)\n",
    "    >>> mdl = GaussianWeightedNewsvendor(cu, co, kernel_bandwidth=10)\n",
    "    >>> mdl.fit(X_train, Y_train)\n",
    "    >>> score(X_test, Y_test)\n",
    "    TODO: Add output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 cu=None,\n",
    "                 co=None,\n",
    "                 kernel_bandwidth=1\n",
    "                 ):\n",
    "        self.kernel_bandwidth = kernel_bandwidth\n",
    "        super().__init__(\n",
    "            cu=cu,\n",
    "            co=co\n",
    "        )\n",
    "\n",
    "    def _get_fitted_model(self, X=None, y=None):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def get_kernel_output_mpmath(self, u):\n",
    "        k_w = mp.exp(-0.5 * math.pow(u / self.kernel_bandwidth, 2))\n",
    "        return k_w\n",
    "    \n",
    "\n",
    "    def get_kernel_output(self, u):\n",
    "        k_w = math.exp(-0.5 * math.pow(u / self.kernel_bandwidth, 2))\n",
    "        return k_w\n",
    "    \n",
    "\n",
    "    def _calc_weights(self, sample):\n",
    "        distances = distance_matrix(self.X_, [sample]).ravel()\n",
    "\n",
    "        distances_kernel_weighted = np.array([self.get_kernel_output(x) for x in distances])\n",
    "        total = np.sum(distances_kernel_weighted)\n",
    "\n",
    "        if total == 0.0:\n",
    "            print(\"Warning: Kernel outputs are zero. Consider using a higher kernel bandwidth.\")\n",
    "            distances_kernel_weighted = np.array([self.get_kernel_output_mpmath(x) for x in distances])\n",
    "            total = np.sum(distances_kernel_weighted)\n",
    "\n",
    "        weights = distances_kernel_weighted / total\n",
    "        \n",
    "        # Actually unnecessary as all weights will be positive for the\n",
    "        # gaussian kernel anyway. Included only for the sake of code consistency.\n",
    "        weightPosIndex = np.where(weights > 0)[0]\n",
    "        weightsPos = weights[weightPosIndex]\n",
    "\n",
    "        return (weightsPos, weightPosIndex)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the estimator from the training set (X,y)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples, n_features)\n",
    "            The training target values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        self : GaussianWeightedNewsvendor\n",
    "            Fitted estimator\n",
    "        \"\"\"\n",
    "\n",
    "        super().fit(X, y)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fdf292-6015-4945-88bf-629440141bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
