{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deccc8a9-8a7b-483c-9379-14db1571b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0c691-8934-45ac-aa8e-458b1ccdadd7",
   "metadata": {},
   "source": [
    "# Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048b4c8-599d-47e6-b7bc-991945e480ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metrics._scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb569ae-5d6c-4839-85f4-15bde82bba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043dda5-8a7b-4530-9448-0a00e38f4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from sklearn.metrics._scorer import _BaseScorer\n",
    "from ddop2.newsvendor import _SampleAverageApproximationNewsvendor as SAA\n",
    "import inspect\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6eb384-2278-4baa-ab49-84a88876d722",
   "metadata": {},
   "source": [
    "## Scorer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cd68b-f8e8-40a9-87d2-a158b5dbfb9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_BaseScorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_Scorer\u001b[39;00m(\u001b[43m_BaseScorer\u001b[49m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_caller, estimator, X, y_true):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124;03m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m        Parameters\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m            Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_BaseScorer' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "class _Scorer(_BaseScorer):\n",
    "    \n",
    "    def _score(self, method_caller, estimator, X, y_true):\n",
    "        \n",
    "        \"\"\"Evaluate predicted target values for X relative to y_true.\n",
    "        Parameters\n",
    "        ----------\n",
    "        method_caller : callable\n",
    "            Returns predictions given an estimator, method name, and other\n",
    "            arguments, potentially caching results.\n",
    "        estimator : object\n",
    "            Trained estimator to use for scoring. Must have a predict_proba\n",
    "            method; the output of that is used to compute the score.\n",
    "        X : {array-like, sparse matrix}\n",
    "            Test data that will be fed to estimator.predict.\n",
    "        y_true : array-like\n",
    "            Gold standard target values for X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Score function applied to prediction of estimator on X.\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = method_caller(estimator, \"predict\", X)\n",
    "        cu = estimator.cu_\n",
    "        co = estimator.co_\n",
    "\n",
    "        if \"y_pred_saa\" in inspect.getfullargspec(self._score_func).args:\n",
    "            X = estimator.X_\n",
    "            y = estimator.y_\n",
    "            y_pred_saa = SAA.SampleAverageApproximationNewsvendor(cu, co).fit(y_true).predict().flatten()\n",
    "            y_pred_saa = np.full(y_true.shape, y_pred_saa)\n",
    "            return self._sign * self._score_func(y_true, y_pred, y_pred_saa, cu, co, **self._kwargs)\n",
    "\n",
    "        else:\n",
    "            print(\"else\")\n",
    "            return self._sign * self._score_func(y_true, y_pred, cu, co, **self._kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b36ba1-e8ec-4a7e-8f92-383467907fc2",
   "metadata": {},
   "source": [
    "## Make Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6e7c7-0dcd-4b98-b7fc-88ed9994d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def make_scorer(score_func, greater_is_better=True, **kwargs):\n",
    "    \n",
    "    \"\"\"Make a scorer from a performance metric or loss function.\n",
    "    This factory function wraps scoring functions for use in\n",
    "    `sklearn.model_selection.GridSearchCV` and\n",
    "    `sklearn.model_selection.cross_val_score`.\n",
    "    It takes a score function from `ddop.metrics`, such as `ddop.metrics.total_costs`,\n",
    "    and returns a callable that scores an estimator's output.\n",
    "    The signature of the call is `(estimator, X, y)` where `estimator`\n",
    "    is the model to be evaluated, `X` is the data and `y` is the\n",
    "    ground truth labeling.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    score_func : callable\n",
    "        Score function included in ddop.metrics.\n",
    "    greater_is_better : bool, default=True\n",
    "        Whether score_func is a score function (default), meaning high is good,\n",
    "        or a loss function, meaning low is good. In the latter case, the\n",
    "        scorer object will sign-flip the outcome of the score_func.\n",
    "    **kwargs : additional arguments\n",
    "        Additional parameters to be passed to score_func.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scorer : callable\n",
    "        Callable object that returns a scalar score; greater is better.\n",
    "    \"\"\"\n",
    "    \n",
    "    sign = 1 if greater_is_better else -1\n",
    "\n",
    "    cls = _Scorer\n",
    "\n",
    "    return cls(score_func, sign, kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf21e30d-9e82-474d-ac0e-b8ea7454dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f43d9-2c0c-4d99-ab28-e25934c70210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dddex",
   "language": "python",
   "name": "dddex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
